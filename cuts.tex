==

The first and primary contribution of this paper is the first experimental test of the validity of this fundametal hypothesis for the (RL-DSTA) method of Bagheri et al.~\cite{}, in particular, with positive results. Their work is based on relational logic specifications and the use of relational logic model finders to enumerate designs in a design space. Carrying out these experiments required a new apparatus for the dynamic profiling of very large numbers of implemented designs, which we specified, implemented, and evaluated, constituting the second contribution of this work. The framework itself was derived by automated extraction from a Coq specification that formalizes in constructive logic our end-to-end system architecture for RL-DSTA, particularly the pipeline taking input specificatoins to output tradespace profiles. The specification is polymorphic in the types of the specification language, semantics, and measurement functions, and those type parameters map to abstract classes in the extracted object-oriented framework.

Building on their work, we carried out a set of experiments to see if their RL-DSTA methods could find better designs than commonly used tools produce in the particular domain of object-relation mapping (ORM). Specifications here denote object-oriented data models. Designs are relational database schemas and object-relational mapping functions. There are many designs (mappings to SQL schemas) possible for any given specification (OO data model). Such designs exhibit significant variations in runtime (read time, write time, and storage space) performance. As a test of the hypothesis, we compared best designs found using exhaustive design space enumerations produced by RL-DSTA against the solution generated quickly and easily for the same specifications using standard commercial ORM tools.  Our experiments yielding highly positive results, revealing designs with radically better performance in {\em all} measured dimensions than those produced by ordinary methods, as well as significant tradeoffs available on the Pareto fronts of these design spaces. 


==

We present evidence from several experiments suggesting that, at least in the domain of object-oriented-to-relational mapping (ORM), RL-DSTA does tend to reveal designs that are are strictly and significantly better than those produced by commercial ORM tools, and finding such designs is practical for modest specifications at scales that are often useful in practice. Carrying out our experiments required a new apparatus: not only to enumerate design models but to convert them to executable implementations, profile their performance under controlled loads using given property measurement procedures, and to handle the resulting data. We call our system {\em Astronaut}. It is a formally specified, general, object-oriented framework for {\em dynamic} RL-DSTA. It is parameterized by domain-specific specification language, semantics, and dynamic property measurement functions, which are provided in concrete subclasses of abstract framework classes. It automates end-to-end conversion of systems specifications into complete tradespace profiles. To test its usability and utility, we specialized it for ORM and used it for our experiments. While bounded relational logic model enumeration is a {\em \#P-complete} problem, thus intractable in general, profiling implementations is trivially parallelizable. We made Astronaut useful by incorporating a Spark-based back-end for parallel profiling of the tens of thousands (or more) of designs found even for modest specifications, reducing dynamic profiling time from weeks on a personal computer to minutes on a cluster.


\section{JUNK}


\section{Introduction}
When the consequences of variations in candidate designs of a given specification are unclear, it can be important to conduct systematic tradeoff studies. Commonly used development tools and methods usually do static analysis or limited dynamic analysis to present one single design in such a space, and ignore other potential better ones.

In previous work~\cite{trademaker}, we presented an approach to (1) synthesizing software design spaces and performance test suites for individual designs within such spaces using relational logic specifications and model finders, and (2) to trade-off analysis of resulting designs using both static performance estimation functions (based on solution structure) and dynamic performance measures (based on profiling of running solutions). The static measures we used were computationally very low cost. We then showed that dynamic analysis of relatively small samples could be used to assess the predictive accuracy of static measures. We settled on sampling the design space for dynamic because dynamic analysis was not only orders of magnitude more computationally expensive but we also lacked technology to automate it. Nevertheless, we were able to show that previously published static estimators of runtime relational database performance not very accurate when compared with baseline results from dynamic measurement of actual performance under load.

Our previous work left an important question unanswered. Our static analysis was inaccurate and could not be relied upon, while our dynamic analysis was incomplete. We were left not fully understanding (1) the actual tradeoff curve for the full design spaces, (2) the characteristics of Pareto-optimal designs in such spaces, and (3) whether our synthesis and tradeoff profiling method has significant potential to reveal software designs that are significantly better than those that typical programmers would produce using standard tools and methods.

The goal of the work reported in this paper was to answer these questions. As noted, we were impeded in answering this question by a lack automated tools for exhaustive and efficient dynamic analysis of large design spaces. We were already able to synthesize candidate solutions along with test loads specialized to the unique interfaces presented by these variants, but we had to run and profile candidate designs by hand. Each run took time ranging from a few seconds to several minutes. There were for some specifications tens of thousands of candidate solutions, of which we profiled carefully selected samples of up to several hundred designs.

What we seek is an end-to-end infrastructure for fully automated tradespace analysis applicable to a range of problem domains. Our vision is that a programmer working in a particular domain provides as input an abstract model of a system and receives as output an understandable map of the multi-dimensional tradespace in the dimensions of interest. [Note: This is hardly a new idea.] Internally, the technology would (1) convert a given abstract model to a relational logic specification; (2) combine such a model with a pre-programmed relational logic specification of a non-deterministic semantics that associates each model with a set of satisfying solutions that constitute candidate designs; (3) use a SAT-based relational logic model finder to enumerate the solutions (exhaustively for tractable problems); (4) automatically apply a suite of static and dynamic property estimation and measurement functions to the resulting candidates; (5) summarize and present the resulting tradeoff data to users in useful forms; (6) deliver fully working code to the user for selected candidate solutions.

Our previous work demonstrated the feasibility of (1), (2), and (3), and of (4) but only for static property estimation functions and only for the particular domain of object-relation mapping (ORM) (where input specifications are object-oriented data models and candidate solutions are SQL database schemas and mappings). We implemented our static estimator functions as ``hardwired'' procedures that operated on relational logic models of database schemas. Our earlier work did address (5) insofar as we presented a web-based tool that presented static analysis results in a potentially useful form. In particular, it shows one representative solution of a set of performance equivalence classes determined by static estimates. The user was thus given a manageable number of solutions from which to select, where each solution associated with a set of static estimates. It was sufficient for our experimental purposes to run this code on an ordinary server. Our work also did address (6) insofar as the tool would deliver instantiated SQL schemas for user-selected candidate solutions. This function required the ability of the tool to translate relational logic models of database schemes into SQL formaet. This functionality was a key enabler of the limited dynamic analysis that we were able to perform. In a nutshell, we extracted actual SQL schemas from the relational logic encoded solutions, executed those SQL scripts to set up the corresponding databases, and then ran and test loads against those databases. Finally, our previously developed technology was able both to synthesize ``abstract'' test loads from given object model specifications and then specialize them into ``concrete'' test loads that match the schema structure of each design. We were thus able to subject all synthesized designs to common test loads for purposes of fair performance comparison.

What our previous work did not accomplish was (1) complete automation of dynamic analysis, (2) making exhaustive dynamic profiling given the orders of magnitude increasing as the analysis time it requires for each solution, (3) actually performing such complete design space profiling for representative problems in the ORM domain, (4) determination as to whether such complete profiling has the potential to reveal significantly better solutions than those that programmers are typically producing using current tools and methods, (5) graphical presentation of the tradespace profiles based on analysis results, (6) generalization of these methods from the domain of ORM to arbitrary domains for which one can provide (a) an end-user modeling language, (b) a translator from that language to an internal domain-specific specification language (IDSL) embedded in relational logic, (c) a non-deterministic semantics for expressions in that IDSL that when combined with an IDSL expression and submitted to a model finder produces solutions that constitute candidate solutions, (d) a suite of property estimation and measurement functions applicable to the solutions so discovered, whether in an internal form.

A satisfactory solution to these unresolved issues would realize the vision we have articulated: a generalized, fully automated, and rigorously formal design space specification and tradeoff profiling system. In this paper, we present the design of such a system and its evaluation against the criteria we have articulate. In particular, it supports complete end-to-end tradespace profiling, using cloud-scale computing (Spark~\cite{spark_paper}) to parallelize dynamic profiling of hundreds of thousands, millions, or more of candidate solutions. It takes user-level models as input, converts them to logical form, synthesizes the designs and test loads, profiles the all designs with these test loads results, summarizes both designs and profiling results, presents it to users in useful form (focusing on Pareto frontiers and unavoidable tradeoffs), and delivers synthesized code from user-selected candidate solutions. In particular, we produced a generic formal specification of the framework in Coq, using type parameters to generalize the key points of variation -- input languages, semantics, and estimation/measurement functions -- and we used Coq extraction to Scala to produce an executable implementation in the form of an object-oriented framework that is easy to specialize by subclass extension.

The principal claim that we make in this paper is that we have designed and implemented a solution that has the characteristics required of a satisfactory solution to the challenges set forth in this work. Specific additional claims that we make for this technology are (1) the specialization extension points support instantiation of the framework for particular domains, (2) it can perform exhaustive dynamic profiling of useful design spaces in the ORM domain, (3) it can find solutions that are better in measured dimensions of time and space performance than those that programmers are producing using current tools and methods, (4) the results produced are readily understandable and actionable.

To evaluate these claims, we instantiated this framework to an automated tool in ORM domain. We used this tool to synthesize SQL schemas and test loads (INSERT and SELECT statements), and dynamically analyzed the time and space performance of each SQL schemas. The analysis results strongly support our claims.

The rest of this paper is structured as follows: Section~\ref{motivation}, motivation; Section~\ref{approach}, the approach overview; Section~\ref{framework} technical details of our framework of tradeoff analysis tools as a hierarchy of typeclasses in Coq; Section~\ref{experiments}, experiments and results; Section~\ref{evaluation}, interpretation and evaluation; Section~\ref{relatedwork}, related work; Section~\ref{discussion}, discussions and conclusions.

\section{Motivation}
\label{motivation}
Software synthesis from formal specifications is a technique for automatically generating program designs or implementations that satisfy a given specification~\cite{Manna:deductive_approach}. Related techniques have seen a resurgence in recent years spurred by growing software complexity and enabled by advances in formal notations and verification technology~\cite{Katebi:2011:SAT}. Synthesis technique has been applied in many kinds of systems, from embedded systems~\cite{Schirner:embedded} to ORM [4]. Synthesis can give rise not just to one solution for a specification but to many, thus supporting a trade-space approach to system design.

In earlier work, researchers developed a synthesis technique in ORM domain that generate SQL schemas from a given specification~\cite{trademaker}. They conducted limited dynamic analysis on these generated schemas to evaluate the predictive accuracy of some published static metrics~\cite{orm_metrics} that estimate the performance of a SQL schema based on the internal structure. They found that some static metrics are not very accurate. For example, one property metric suggests that some schemas are not Pareto-optimal due to they include too many NULL values, which leads to too much space consumption. However, dynamic runtime profiling revealed that MySQL database engine was doing a very good job in optimizing away storage of NULL values. Dynamic analysis is tedious and time-consuming. It took two person months of effort to run the experiments on only some representative schemas.

The untrustworthiness of the static metrics motivated us to try to conduct exhaustive dynamic analysis on all generated schemas. Exhaustive analysis serves several purposes. First, it provides better data to assess the predictive power of the static measures. Second, the results could be used to learn and validate better predictors. Third, it provides a basis for engineers to better understand the tradeoff spaces for their systems, and to make better design decisions. Finally, it supports experiments to help answer the key question left open by earlier work: does synthesis + tradeoff analysis have the potential to produce better designs, with respect to desired non-functional property, than those produce by commonly used tools and methods?

However, it is impossible to carry out dynamic analysis manually even for an object model in modest size. The number of designs could be exponential order with respect to the number of objects and the relationship among them. To conduct such a tradeoff analysis, we need tool that is fully automated, hopefully generalized.

The unaccurate of static metrics and need in tool support to carry out dynamic tradeoff analysis motivated us to perform this work.

\section{Approach Overview}
\label{approach}

In this section we describe how our overall approach that leverages advances in several areas of computer science and engineering to serve our synthesis + dynamic analysis approach to discover better designs.

\subsection{Constructive Logic Proof Assistants}
First, we use the enormous expressiveness of dependent type theory in modern constructive logic proof assistants to produce precise, abstract, general, computationally effective theories of domains such as tradeoff analysis. We present an algebraic theory of tradeoff analysis tools structured as a hierarchy of Coq~\cite{coq_book} Typeclasses, in a style similar to that being used by mathematicians~\cite{spitters2011type, pelayo2014homotopy} to formalize hierarchies of abstract algebraic structures (e.g., groups, fields, topological spaces). \textbf{Chong: Why constructive logic? Easy to prototype or something else?}

\subsection{Certified Programming with Dependent Types}
Second, we use Coq's extraction facility to derive a certified~\cite{chlipala2013certified} implementation of a general-purpose framework (in Scala). \textbf{Any merit when to model this framework first and then extract to Scala? Why don't use Scala from the beginning?} It is then specialized and extended with user-defined, domain-specific types and functions (and in some cases, proofs), subject to the specified laws, to create domain-specific analysis tools. The theory provides implementations of functions common to all instances and expresses laws that all instances must obey.

\subsection{Formal Synthesis from Specifications}
Third, as Bagheri et al.~\cite{trademaker}, Dwivedi et al.~\cite{dwivedi2014model}, and others have been showing, one can increasingly synthesize many implementations from given specifications, particularly for tradeoff analysis. Bagheri~\cite{trademaker} recently showed that a relational logic model finder can exhaustively synthesize relational database schemas as well as test loads for dynamic analysis of performance from relational logic specifications of object-oriented data models~\cite{spacemaker, trademaker}.

Third, software synthesis techniques have seen a resurgence in the recent years spurred by growth in software complexity and enabled by advances in formal notations and verification technology~\cite{Katebi:2011:SAT}. These techniques have been applied in many kinds of systems, from embedded systems~\cite{Schirner:embedded} to database designs~\cite{spacemaker} and software architecture~\cite{dwivedi2014model}. Synthesis can give rise not just to one solution for a specification but to many, thus supporting a tradespace approach to software system design.

Our framework is meant to be specialized using any types of software synthesis techniques. When implementation synthesizers are available, they should be easy to plug in. When they are not, other hand-crafted functions can be used. To test this idea, we re-engineered and extended an Alloy-based~\cite{daniel_jackson_alloy:lightweight_2002} ORM synthesizer, where a relational logic model finder is used to exhaustively synthesize relational database schemas as well as test loads for dynamic analysis of performance from relational logic specifications of object-oriented data models~\cite{spacemaker, trademaker}, to produce a fully automated, synthesis-driven ORM tradespace analysis tool.

With this tool, we are able to fully replicate the largely manual analysis of synthesized database schemas in earlier work~\cite{trademaker}. The tool works with far higher reliability, and is just one of many possible specialized instance of a general, theory-based framework. One can now rapidly produce tool variants. Our tool has reduced the time required to analyze thousands of candidate solutions from weeks, involving tedious manual execution of synthesized benchmarks, to just hours.

\subsection{Scalable ``Big Data'' Analytics}
Fourth, we use big data analytics, particularly map-reduce~\cite{dean2008mapreduce}, to reduce analysis runtimes. Using the Alloy Analyzer to synthesize designs from specifications did not parallelize, but applying property measurement functions to each design did. Applying each measurement function to each implementation has a natural map-reduce structure. The use of scalable map-reduce technology can benefit many instances of our framework, so it is sensible to support it as a common middleware plug-in.


